2023-09-02 11:18:22,551 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/configuration_utils.py[line:254] - INFO: loading configuration file ./gpt2-small/config.json
2023-09-02 11:18:22,552 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/configuration_utils.py[line:292] - INFO: Model config GPT2Config {
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": null,
  "context_size": 2,
  "do_sample": false,
  "embd_pdrop": 0.1,
  "eos_token_ids": null,
  "finetuning_task": null,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_epsilon": 1e-05,
  "length_penalty": 1.0,
  "max_length": 20,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 12,
  "n_positions": 1024,
  "num_beams": 1,
  "num_labels": 2,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": null,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "resid_pdrop": 0.1,
  "shared_attention": false,
  "shared_module": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "use_bfloat16": false,
  "vocab_size": 50257
}

2023-09-02 11:18:22,552 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/modeling_utils.py[line:459] - INFO: loading weights file ./gpt2-small/pytorch_model.bin
2023-09-02 11:18:33,374 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/modeling_utils.py[line:546] - INFO: Weights of GPT2EncoderDecoderModel not initialized from pretrained model: ['transformer.h.0.context_attns.0.bias', 'transformer.h.0.context_attns.0.c_attn.weight', 'transformer.h.0.context_attns.0.c_attn.bias', 'transformer.h.0.context_attns.0.c_proj.weight', 'transformer.h.0.context_attns.0.c_proj.bias', 'transformer.h.0.context_attns.1.bias', 'transformer.h.0.context_attns.1.c_attn.weight', 'transformer.h.0.context_attns.1.c_attn.bias', 'transformer.h.0.context_attns.1.c_proj.weight', 'transformer.h.0.context_attns.1.c_proj.bias', 'transformer.h.0.attention_module.bias', 'transformer.h.0.attention_module.c_proj.weight', 'transformer.h.0.attention_module.c_proj.bias', 'transformer.h.1.context_attns.0.bias', 'transformer.h.1.context_attns.0.c_attn.weight', 'transformer.h.1.context_attns.0.c_attn.bias', 'transformer.h.1.context_attns.0.c_proj.weight', 'transformer.h.1.context_attns.0.c_proj.bias', 'transformer.h.1.context_attns.1.bias', 'transformer.h.1.context_attns.1.c_attn.weight', 'transformer.h.1.context_attns.1.c_attn.bias', 'transformer.h.1.context_attns.1.c_proj.weight', 'transformer.h.1.context_attns.1.c_proj.bias', 'transformer.h.1.attention_module.bias', 'transformer.h.1.attention_module.c_proj.weight', 'transformer.h.1.attention_module.c_proj.bias', 'transformer.h.2.context_attns.0.bias', 'transformer.h.2.context_attns.0.c_attn.weight', 'transformer.h.2.context_attns.0.c_attn.bias', 'transformer.h.2.context_attns.0.c_proj.weight', 'transformer.h.2.context_attns.0.c_proj.bias', 'transformer.h.2.context_attns.1.bias', 'transformer.h.2.context_attns.1.c_attn.weight', 'transformer.h.2.context_attns.1.c_attn.bias', 'transformer.h.2.context_attns.1.c_proj.weight', 'transformer.h.2.context_attns.1.c_proj.bias', 'transformer.h.2.attention_module.bias', 'transformer.h.2.attention_module.c_proj.weight', 'transformer.h.2.attention_module.c_proj.bias', 'transformer.h.3.context_attns.0.bias', 'transformer.h.3.context_attns.0.c_attn.weight', 'transformer.h.3.context_attns.0.c_attn.bias', 'transformer.h.3.context_attns.0.c_proj.weight', 'transformer.h.3.context_attns.0.c_proj.bias', 'transformer.h.3.context_attns.1.bias', 'transformer.h.3.context_attns.1.c_attn.weight', 'transformer.h.3.context_attns.1.c_attn.bias', 'transformer.h.3.context_attns.1.c_proj.weight', 'transformer.h.3.context_attns.1.c_proj.bias', 'transformer.h.3.attention_module.bias', 'transformer.h.3.attention_module.c_proj.weight', 'transformer.h.3.attention_module.c_proj.bias', 'transformer.h.4.context_attns.0.bias', 'transformer.h.4.context_attns.0.c_attn.weight', 'transformer.h.4.context_attns.0.c_attn.bias', 'transformer.h.4.context_attns.0.c_proj.weight', 'transformer.h.4.context_attns.0.c_proj.bias', 'transformer.h.4.context_attns.1.bias', 'transformer.h.4.context_attns.1.c_attn.weight', 'transformer.h.4.context_attns.1.c_attn.bias', 'transformer.h.4.context_attns.1.c_proj.weight', 'transformer.h.4.context_attns.1.c_proj.bias', 'transformer.h.4.attention_module.bias', 'transformer.h.4.attention_module.c_proj.weight', 'transformer.h.4.attention_module.c_proj.bias', 'transformer.h.5.context_attns.0.bias', 'transformer.h.5.context_attns.0.c_attn.weight', 'transformer.h.5.context_attns.0.c_attn.bias', 'transformer.h.5.context_attns.0.c_proj.weight', 'transformer.h.5.context_attns.0.c_proj.bias', 'transformer.h.5.context_attns.1.bias', 'transformer.h.5.context_attns.1.c_attn.weight', 'transformer.h.5.context_attns.1.c_attn.bias', 'transformer.h.5.context_attns.1.c_proj.weight', 'transformer.h.5.context_attns.1.c_proj.bias', 'transformer.h.5.attention_module.bias', 'transformer.h.5.attention_module.c_proj.weight', 'transformer.h.5.attention_module.c_proj.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.6.context_attns.0.bias', 'transformer.h.6.context_attns.0.c_attn.weight', 'transformer.h.6.context_attns.0.c_attn.bias', 'transformer.h.6.context_attns.0.c_proj.weight', 'transformer.h.6.context_attns.0.c_proj.bias', 'transformer.h.6.context_attns.1.bias', 'transformer.h.6.context_attns.1.c_attn.weight', 'transformer.h.6.context_attns.1.c_attn.bias', 'transformer.h.6.context_attns.1.c_proj.weight', 'transformer.h.6.context_attns.1.c_proj.bias', 'transformer.h.6.attention_module.bias', 'transformer.h.6.attention_module.c_proj.weight', 'transformer.h.6.attention_module.c_proj.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.7.context_attns.0.bias', 'transformer.h.7.context_attns.0.c_attn.weight', 'transformer.h.7.context_attns.0.c_attn.bias', 'transformer.h.7.context_attns.0.c_proj.weight', 'transformer.h.7.context_attns.0.c_proj.bias', 'transformer.h.7.context_attns.1.bias', 'transformer.h.7.context_attns.1.c_attn.weight', 'transformer.h.7.context_attns.1.c_attn.bias', 'transformer.h.7.context_attns.1.c_proj.weight', 'transformer.h.7.context_attns.1.c_proj.bias', 'transformer.h.7.attention_module.bias', 'transformer.h.7.attention_module.c_proj.weight', 'transformer.h.7.attention_module.c_proj.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.8.context_attns.0.bias', 'transformer.h.8.context_attns.0.c_attn.weight', 'transformer.h.8.context_attns.0.c_attn.bias', 'transformer.h.8.context_attns.0.c_proj.weight', 'transformer.h.8.context_attns.0.c_proj.bias', 'transformer.h.8.context_attns.1.bias', 'transformer.h.8.context_attns.1.c_attn.weight', 'transformer.h.8.context_attns.1.c_attn.bias', 'transformer.h.8.context_attns.1.c_proj.weight', 'transformer.h.8.context_attns.1.c_proj.bias', 'transformer.h.8.attention_module.bias', 'transformer.h.8.attention_module.c_proj.weight', 'transformer.h.8.attention_module.c_proj.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.9.context_attns.0.bias', 'transformer.h.9.context_attns.0.c_attn.weight', 'transformer.h.9.context_attns.0.c_attn.bias', 'transformer.h.9.context_attns.0.c_proj.weight', 'transformer.h.9.context_attns.0.c_proj.bias', 'transformer.h.9.context_attns.1.bias', 'transformer.h.9.context_attns.1.c_attn.weight', 'transformer.h.9.context_attns.1.c_attn.bias', 'transformer.h.9.context_attns.1.c_proj.weight', 'transformer.h.9.context_attns.1.c_proj.bias', 'transformer.h.9.attention_module.bias', 'transformer.h.9.attention_module.c_proj.weight', 'transformer.h.9.attention_module.c_proj.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.10.context_attns.0.bias', 'transformer.h.10.context_attns.0.c_attn.weight', 'transformer.h.10.context_attns.0.c_attn.bias', 'transformer.h.10.context_attns.0.c_proj.weight', 'transformer.h.10.context_attns.0.c_proj.bias', 'transformer.h.10.context_attns.1.bias', 'transformer.h.10.context_attns.1.c_attn.weight', 'transformer.h.10.context_attns.1.c_attn.bias', 'transformer.h.10.context_attns.1.c_proj.weight', 'transformer.h.10.context_attns.1.c_proj.bias', 'transformer.h.10.attention_module.bias', 'transformer.h.10.attention_module.c_proj.weight', 'transformer.h.10.attention_module.c_proj.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.11.context_attns.0.bias', 'transformer.h.11.context_attns.0.c_attn.weight', 'transformer.h.11.context_attns.0.c_attn.bias', 'transformer.h.11.context_attns.0.c_proj.weight', 'transformer.h.11.context_attns.0.c_proj.bias', 'transformer.h.11.context_attns.1.bias', 'transformer.h.11.context_attns.1.c_attn.weight', 'transformer.h.11.context_attns.1.c_attn.bias', 'transformer.h.11.context_attns.1.c_proj.weight', 'transformer.h.11.context_attns.1.c_proj.bias', 'transformer.h.11.attention_module.bias', 'transformer.h.11.attention_module.c_proj.weight', 'transformer.h.11.attention_module.c_proj.bias', 'encoder.wte.weight', 'encoder.wpe.weight', 'encoder.h.0.ln_1.weight', 'encoder.h.0.ln_1.bias', 'encoder.h.0.attn.bias', 'encoder.h.0.attn.c_attn.weight', 'encoder.h.0.attn.c_attn.bias', 'encoder.h.0.attn.c_proj.weight', 'encoder.h.0.attn.c_proj.bias', 'encoder.h.0.ln_2.weight', 'encoder.h.0.ln_2.bias', 'encoder.h.0.mlp.c_fc.weight', 'encoder.h.0.mlp.c_fc.bias', 'encoder.h.0.mlp.c_proj.weight', 'encoder.h.0.mlp.c_proj.bias', 'encoder.h.0.context_attns.0.bias', 'encoder.h.0.context_attns.0.c_attn.weight', 'encoder.h.0.context_attns.0.c_attn.bias', 'encoder.h.0.context_attns.0.c_proj.weight', 'encoder.h.0.context_attns.0.c_proj.bias', 'encoder.h.0.context_attns.1.bias', 'encoder.h.0.context_attns.1.c_attn.weight', 'encoder.h.0.context_attns.1.c_attn.bias', 'encoder.h.0.context_attns.1.c_proj.weight', 'encoder.h.0.context_attns.1.c_proj.bias', 'encoder.h.0.attention_module.bias', 'encoder.h.0.attention_module.c_proj.weight', 'encoder.h.0.attention_module.c_proj.bias', 'encoder.h.1.ln_1.weight', 'encoder.h.1.ln_1.bias', 'encoder.h.1.attn.bias', 'encoder.h.1.attn.c_attn.weight', 'encoder.h.1.attn.c_attn.bias', 'encoder.h.1.attn.c_proj.weight', 'encoder.h.1.attn.c_proj.bias', 'encoder.h.1.ln_2.weight', 'encoder.h.1.ln_2.bias', 'encoder.h.1.mlp.c_fc.weight', 'encoder.h.1.mlp.c_fc.bias', 'encoder.h.1.mlp.c_proj.weight', 'encoder.h.1.mlp.c_proj.bias', 'encoder.h.1.context_attns.0.bias', 'encoder.h.1.context_attns.0.c_attn.weight', 'encoder.h.1.context_attns.0.c_attn.bias', 'encoder.h.1.context_attns.0.c_proj.weight', 'encoder.h.1.context_attns.0.c_proj.bias', 'encoder.h.1.context_attns.1.bias', 'encoder.h.1.context_attns.1.c_attn.weight', 'encoder.h.1.context_attns.1.c_attn.bias', 'encoder.h.1.context_attns.1.c_proj.weight', 'encoder.h.1.context_attns.1.c_proj.bias', 'encoder.h.1.attention_module.bias', 'encoder.h.1.attention_module.c_proj.weight', 'encoder.h.1.attention_module.c_proj.bias', 'encoder.h.2.ln_1.weight', 'encoder.h.2.ln_1.bias', 'encoder.h.2.attn.bias', 'encoder.h.2.attn.c_attn.weight', 'encoder.h.2.attn.c_attn.bias', 'encoder.h.2.attn.c_proj.weight', 'encoder.h.2.attn.c_proj.bias', 'encoder.h.2.ln_2.weight', 'encoder.h.2.ln_2.bias', 'encoder.h.2.mlp.c_fc.weight', 'encoder.h.2.mlp.c_fc.bias', 'encoder.h.2.mlp.c_proj.weight', 'encoder.h.2.mlp.c_proj.bias', 'encoder.h.2.context_attns.0.bias', 'encoder.h.2.context_attns.0.c_attn.weight', 'encoder.h.2.context_attns.0.c_attn.bias', 'encoder.h.2.context_attns.0.c_proj.weight', 'encoder.h.2.context_attns.0.c_proj.bias', 'encoder.h.2.context_attns.1.bias', 'encoder.h.2.context_attns.1.c_attn.weight', 'encoder.h.2.context_attns.1.c_attn.bias', 'encoder.h.2.context_attns.1.c_proj.weight', 'encoder.h.2.context_attns.1.c_proj.bias', 'encoder.h.2.attention_module.bias', 'encoder.h.2.attention_module.c_proj.weight', 'encoder.h.2.attention_module.c_proj.bias', 'encoder.h.3.ln_1.weight', 'encoder.h.3.ln_1.bias', 'encoder.h.3.attn.bias', 'encoder.h.3.attn.c_attn.weight', 'encoder.h.3.attn.c_attn.bias', 'encoder.h.3.attn.c_proj.weight', 'encoder.h.3.attn.c_proj.bias', 'encoder.h.3.ln_2.weight', 'encoder.h.3.ln_2.bias', 'encoder.h.3.mlp.c_fc.weight', 'encoder.h.3.mlp.c_fc.bias', 'encoder.h.3.mlp.c_proj.weight', 'encoder.h.3.mlp.c_proj.bias', 'encoder.h.3.context_attns.0.bias', 'encoder.h.3.context_attns.0.c_attn.weight', 'encoder.h.3.context_attns.0.c_attn.bias', 'encoder.h.3.context_attns.0.c_proj.weight', 'encoder.h.3.context_attns.0.c_proj.bias', 'encoder.h.3.context_attns.1.bias', 'encoder.h.3.context_attns.1.c_attn.weight', 'encoder.h.3.context_attns.1.c_attn.bias', 'encoder.h.3.context_attns.1.c_proj.weight', 'encoder.h.3.context_attns.1.c_proj.bias', 'encoder.h.3.attention_module.bias', 'encoder.h.3.attention_module.c_proj.weight', 'encoder.h.3.attention_module.c_proj.bias', 'encoder.h.4.ln_1.weight', 'encoder.h.4.ln_1.bias', 'encoder.h.4.attn.bias', 'encoder.h.4.attn.c_attn.weight', 'encoder.h.4.attn.c_attn.bias', 'encoder.h.4.attn.c_proj.weight', 'encoder.h.4.attn.c_proj.bias', 'encoder.h.4.ln_2.weight', 'encoder.h.4.ln_2.bias', 'encoder.h.4.mlp.c_fc.weight', 'encoder.h.4.mlp.c_fc.bias', 'encoder.h.4.mlp.c_proj.weight', 'encoder.h.4.mlp.c_proj.bias', 'encoder.h.4.context_attns.0.bias', 'encoder.h.4.context_attns.0.c_attn.weight', 'encoder.h.4.context_attns.0.c_attn.bias', 'encoder.h.4.context_attns.0.c_proj.weight', 'encoder.h.4.context_attns.0.c_proj.bias', 'encoder.h.4.context_attns.1.bias', 'encoder.h.4.context_attns.1.c_attn.weight', 'encoder.h.4.context_attns.1.c_attn.bias', 'encoder.h.4.context_attns.1.c_proj.weight', 'encoder.h.4.context_attns.1.c_proj.bias', 'encoder.h.4.attention_module.bias', 'encoder.h.4.attention_module.c_proj.weight', 'encoder.h.4.attention_module.c_proj.bias', 'encoder.h.5.ln_1.weight', 'encoder.h.5.ln_1.bias', 'encoder.h.5.attn.bias', 'encoder.h.5.attn.c_attn.weight', 'encoder.h.5.attn.c_attn.bias', 'encoder.h.5.attn.c_proj.weight', 'encoder.h.5.attn.c_proj.bias', 'encoder.h.5.ln_2.weight', 'encoder.h.5.ln_2.bias', 'encoder.h.5.mlp.c_fc.weight', 'encoder.h.5.mlp.c_fc.bias', 'encoder.h.5.mlp.c_proj.weight', 'encoder.h.5.mlp.c_proj.bias', 'encoder.h.5.context_attns.0.bias', 'encoder.h.5.context_attns.0.c_attn.weight', 'encoder.h.5.context_attns.0.c_attn.bias', 'encoder.h.5.context_attns.0.c_proj.weight', 'encoder.h.5.context_attns.0.c_proj.bias', 'encoder.h.5.context_attns.1.bias', 'encoder.h.5.context_attns.1.c_attn.weight', 'encoder.h.5.context_attns.1.c_attn.bias', 'encoder.h.5.context_attns.1.c_proj.weight', 'encoder.h.5.context_attns.1.c_proj.bias', 'encoder.h.5.attention_module.bias', 'encoder.h.5.attention_module.c_proj.weight', 'encoder.h.5.attention_module.c_proj.bias', 'encoder.h.6.ln_1.weight', 'encoder.h.6.ln_1.bias', 'encoder.h.6.attn.bias', 'encoder.h.6.attn.c_attn.weight', 'encoder.h.6.attn.c_attn.bias', 'encoder.h.6.attn.c_proj.weight', 'encoder.h.6.attn.c_proj.bias', 'encoder.h.6.ln_2.weight', 'encoder.h.6.ln_2.bias', 'encoder.h.6.mlp.c_fc.weight', 'encoder.h.6.mlp.c_fc.bias', 'encoder.h.6.mlp.c_proj.weight', 'encoder.h.6.mlp.c_proj.bias', 'encoder.h.6.context_attns.0.bias', 'encoder.h.6.context_attns.0.c_attn.weight', 'encoder.h.6.context_attns.0.c_attn.bias', 'encoder.h.6.context_attns.0.c_proj.weight', 'encoder.h.6.context_attns.0.c_proj.bias', 'encoder.h.6.context_attns.1.bias', 'encoder.h.6.context_attns.1.c_attn.weight', 'encoder.h.6.context_attns.1.c_attn.bias', 'encoder.h.6.context_attns.1.c_proj.weight', 'encoder.h.6.context_attns.1.c_proj.bias', 'encoder.h.6.attention_module.bias', 'encoder.h.6.attention_module.c_proj.weight', 'encoder.h.6.attention_module.c_proj.bias', 'encoder.h.7.ln_1.weight', 'encoder.h.7.ln_1.bias', 'encoder.h.7.attn.bias', 'encoder.h.7.attn.c_attn.weight', 'encoder.h.7.attn.c_attn.bias', 'encoder.h.7.attn.c_proj.weight', 'encoder.h.7.attn.c_proj.bias', 'encoder.h.7.ln_2.weight', 'encoder.h.7.ln_2.bias', 'encoder.h.7.mlp.c_fc.weight', 'encoder.h.7.mlp.c_fc.bias', 'encoder.h.7.mlp.c_proj.weight', 'encoder.h.7.mlp.c_proj.bias', 'encoder.h.7.context_attns.0.bias', 'encoder.h.7.context_attns.0.c_attn.weight', 'encoder.h.7.context_attns.0.c_attn.bias', 'encoder.h.7.context_attns.0.c_proj.weight', 'encoder.h.7.context_attns.0.c_proj.bias', 'encoder.h.7.context_attns.1.bias', 'encoder.h.7.context_attns.1.c_attn.weight', 'encoder.h.7.context_attns.1.c_attn.bias', 'encoder.h.7.context_attns.1.c_proj.weight', 'encoder.h.7.context_attns.1.c_proj.bias', 'encoder.h.7.attention_module.bias', 'encoder.h.7.attention_module.c_proj.weight', 'encoder.h.7.attention_module.c_proj.bias', 'encoder.h.8.ln_1.weight', 'encoder.h.8.ln_1.bias', 'encoder.h.8.attn.bias', 'encoder.h.8.attn.c_attn.weight', 'encoder.h.8.attn.c_attn.bias', 'encoder.h.8.attn.c_proj.weight', 'encoder.h.8.attn.c_proj.bias', 'encoder.h.8.ln_2.weight', 'encoder.h.8.ln_2.bias', 'encoder.h.8.mlp.c_fc.weight', 'encoder.h.8.mlp.c_fc.bias', 'encoder.h.8.mlp.c_proj.weight', 'encoder.h.8.mlp.c_proj.bias', 'encoder.h.8.context_attns.0.bias', 'encoder.h.8.context_attns.0.c_attn.weight', 'encoder.h.8.context_attns.0.c_attn.bias', 'encoder.h.8.context_attns.0.c_proj.weight', 'encoder.h.8.context_attns.0.c_proj.bias', 'encoder.h.8.context_attns.1.bias', 'encoder.h.8.context_attns.1.c_attn.weight', 'encoder.h.8.context_attns.1.c_attn.bias', 'encoder.h.8.context_attns.1.c_proj.weight', 'encoder.h.8.context_attns.1.c_proj.bias', 'encoder.h.8.attention_module.bias', 'encoder.h.8.attention_module.c_proj.weight', 'encoder.h.8.attention_module.c_proj.bias', 'encoder.h.9.ln_1.weight', 'encoder.h.9.ln_1.bias', 'encoder.h.9.attn.bias', 'encoder.h.9.attn.c_attn.weight', 'encoder.h.9.attn.c_attn.bias', 'encoder.h.9.attn.c_proj.weight', 'encoder.h.9.attn.c_proj.bias', 'encoder.h.9.ln_2.weight', 'encoder.h.9.ln_2.bias', 'encoder.h.9.mlp.c_fc.weight', 'encoder.h.9.mlp.c_fc.bias', 'encoder.h.9.mlp.c_proj.weight', 'encoder.h.9.mlp.c_proj.bias', 'encoder.h.9.context_attns.0.bias', 'encoder.h.9.context_attns.0.c_attn.weight', 'encoder.h.9.context_attns.0.c_attn.bias', 'encoder.h.9.context_attns.0.c_proj.weight', 'encoder.h.9.context_attns.0.c_proj.bias', 'encoder.h.9.context_attns.1.bias', 'encoder.h.9.context_attns.1.c_attn.weight', 'encoder.h.9.context_attns.1.c_attn.bias', 'encoder.h.9.context_attns.1.c_proj.weight', 'encoder.h.9.context_attns.1.c_proj.bias', 'encoder.h.9.attention_module.bias', 'encoder.h.9.attention_module.c_proj.weight', 'encoder.h.9.attention_module.c_proj.bias', 'encoder.h.10.ln_1.weight', 'encoder.h.10.ln_1.bias', 'encoder.h.10.attn.bias', 'encoder.h.10.attn.c_attn.weight', 'encoder.h.10.attn.c_attn.bias', 'encoder.h.10.attn.c_proj.weight', 'encoder.h.10.attn.c_proj.bias', 'encoder.h.10.ln_2.weight', 'encoder.h.10.ln_2.bias', 'encoder.h.10.mlp.c_fc.weight', 'encoder.h.10.mlp.c_fc.bias', 'encoder.h.10.mlp.c_proj.weight', 'encoder.h.10.mlp.c_proj.bias', 'encoder.h.10.context_attns.0.bias', 'encoder.h.10.context_attns.0.c_attn.weight', 'encoder.h.10.context_attns.0.c_attn.bias', 'encoder.h.10.context_attns.0.c_proj.weight', 'encoder.h.10.context_attns.0.c_proj.bias', 'encoder.h.10.context_attns.1.bias', 'encoder.h.10.context_attns.1.c_attn.weight', 'encoder.h.10.context_attns.1.c_attn.bias', 'encoder.h.10.context_attns.1.c_proj.weight', 'encoder.h.10.context_attns.1.c_proj.bias', 'encoder.h.10.attention_module.bias', 'encoder.h.10.attention_module.c_proj.weight', 'encoder.h.10.attention_module.c_proj.bias', 'encoder.h.11.ln_1.weight', 'encoder.h.11.ln_1.bias', 'encoder.h.11.attn.bias', 'encoder.h.11.attn.c_attn.weight', 'encoder.h.11.attn.c_attn.bias', 'encoder.h.11.attn.c_proj.weight', 'encoder.h.11.attn.c_proj.bias', 'encoder.h.11.ln_2.weight', 'encoder.h.11.ln_2.bias', 'encoder.h.11.mlp.c_fc.weight', 'encoder.h.11.mlp.c_fc.bias', 'encoder.h.11.mlp.c_proj.weight', 'encoder.h.11.mlp.c_proj.bias', 'encoder.h.11.context_attns.0.bias', 'encoder.h.11.context_attns.0.c_attn.weight', 'encoder.h.11.context_attns.0.c_attn.bias', 'encoder.h.11.context_attns.0.c_proj.weight', 'encoder.h.11.context_attns.0.c_proj.bias', 'encoder.h.11.context_attns.1.bias', 'encoder.h.11.context_attns.1.c_attn.weight', 'encoder.h.11.context_attns.1.c_attn.bias', 'encoder.h.11.context_attns.1.c_proj.weight', 'encoder.h.11.context_attns.1.c_proj.bias', 'encoder.h.11.attention_module.bias', 'encoder.h.11.attention_module.c_proj.weight', 'encoder.h.11.attention_module.c_proj.bias', 'encoder.ln_f.weight', 'encoder.ln_f.bias', 'multiple_choice_head.summary.weight', 'multiple_choice_head.summary.bias']
2023-09-02 11:18:33,449 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:417] - INFO: Model name './gpt2-small' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming './gpt2-small' is a path, a model identifier, or url to a directory containing tokenizer files.
2023-09-02 11:18:33,451 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:446] - INFO: Didn't find file ./gpt2-small/added_tokens.json. We won't load it.
2023-09-02 11:18:33,451 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:446] - INFO: Didn't find file ./gpt2-small/special_tokens_map.json. We won't load it.
2023-09-02 11:18:33,451 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:446] - INFO: Didn't find file ./gpt2-small/tokenizer_config.json. We won't load it.
2023-09-02 11:18:33,452 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:499] - INFO: loading file ./gpt2-small/vocab.json
2023-09-02 11:18:33,452 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:499] - INFO: loading file ./gpt2-small/merges.txt
2023-09-02 11:18:33,452 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:499] - INFO: loading file None
2023-09-02 11:18:33,452 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:499] - INFO: loading file None
2023-09-02 11:18:33,453 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:499] - INFO: loading file None
2023-09-02 11:18:33,673 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:660] - INFO: Adding <pad> to the vocabulary
2023-09-02 11:18:33,674 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:738] - INFO: Assigning <pad> to the pad_token key of the tokenizer
2023-09-02 11:18:33,674 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:660] - INFO: Adding <bos> to the vocabulary
2023-09-02 11:18:33,674 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:738] - INFO: Assigning <bos> to the bos_token key of the tokenizer
2023-09-02 11:18:33,674 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:660] - INFO: Adding <eos> to the vocabulary
2023-09-02 11:18:33,675 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:738] - INFO: Assigning <eos> to the eos_token key of the tokenizer
2023-09-02 11:18:33,675 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:660] - INFO: Adding <info_bos> to the vocabulary
2023-09-02 11:18:33,675 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:660] - INFO: Adding <info_eos> to the vocabulary
2023-09-02 11:18:33,675 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:660] - INFO: Adding <talker1_bos> to the vocabulary
2023-09-02 11:18:33,675 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:660] - INFO: Adding <talker1_eos> to the vocabulary
2023-09-02 11:18:33,676 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:660] - INFO: Adding <talker2_bos> to the vocabulary
2023-09-02 11:18:33,676 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:660] - INFO: Adding <talker2_eos> to the vocabulary
2023-09-02 11:18:33,676 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:738] - INFO: Assigning ['<info_bos>', '<info_eos>', '<talker1_bos>', '<talker1_eos>', '<talker2_bos>', '<talker2_eos>'] to the additional_special_tokens key of the tokenizer
2023-09-02 11:18:34,820 - /content/drive/MyDrive/SimOAP/Multi-GPT2/inference.py[line:217] - INFO: loading datasets
2023-09-02 11:19:10,925 - /content/drive/MyDrive/SimOAP/Multi-GPT2/inference.py[line:242] - INFO: valid dataset 7801 test dataset 7512
2023-09-02 11:19:17,686 - /content/drive/MyDrive/SimOAP/Multi-GPT2/inference.py[line:258] - INFO: Weights loaded from ./test/best_model
2023-09-02 11:19:17,686 - /content/drive/MyDrive/SimOAP/Multi-GPT2/model/trainer.py[line:56] - INFO: device: cuda, distributed training: False, apex_level: None, apex_scale_loss: None,  n_gpu: 1
2023-09-02 11:19:17,693 - /content/drive/MyDrive/SimOAP/Multi-GPT2/model/trainer.py[line:673] - INFO: Starting testing on Test dataset
2023-09-02 11:19:22,250 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:501] - INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-vocab.json from cache at /root/.cache/torch/transformers/f6765b3a0e238688a516e885aeadef44f4d34da95f8a5a07793d4fbf0618f92d.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b
2023-09-02 11:19:22,251 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/tokenization_utils.py[line:501] - INFO: loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-merges.txt from cache at /root/.cache/torch/transformers/d9fc1956a01fe24af529f239031a439661e7634e6e931eaad2393db3ae1eff03.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
2023-09-02 11:19:22,551 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/configuration_utils.py[line:256] - INFO: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-config.json from cache at /root/.cache/torch/transformers/54eef9bf74f919edd81b765fee413c8229620f3e271a51bdcdc67797422ef3f3.9334699bb73c7efd2d3a0b84c3dbc8fe2940ec1feeca441399509edb1505f73a
2023-09-02 11:19:22,551 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/configuration_utils.py[line:292] - INFO: Model config RobertaConfig {
  "_num_labels": 3,
  "architectures": [
    "RobertaForSequenceClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "do_sample": false,
  "eos_token_id": 2,
  "eos_token_ids": null,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "CONTRADICTION",
    "1": "NEUTRAL",
    "2": "ENTAILMENT"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "is_decoder": false,
  "label2id": {
    "CONTRADICTION": 0,
    "ENTAILMENT": 2,
    "NEUTRAL": 1
  },
  "layer_norm_eps": 1e-05,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_beams": 1,
  "num_hidden_layers": 24,
  "num_labels": 3,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 1,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

2023-09-02 11:19:22,696 - /usr/local/envs/multigpt/lib/python3.7/site-packages/transformers/modeling_utils.py[line:461] - INFO: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-pytorch_model.bin from cache at /root/.cache/torch/transformers/1c2e185bc053ae7261ce2289653438a4c05b871ff7f30eaee1cdb787154410e0.b8d9b06a213d6baaea1e958d00e5f10c0ea27933a821970ae67888c9b4568322
